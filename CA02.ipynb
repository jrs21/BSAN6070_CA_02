{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vYLW40jgVfAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9afc0b46-0bb9-47f5-a7c7-74e87199ead7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here we are importing the necessarry packages to run the code as intended\n"
          ]
        }
      ],
      "source": [
        "\"\"\"naive_bayes.ipynb\"\"\"\n",
        "print(\"Here we are importing the necessarry packages to run the code as intended\")\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Here we are creating a function that builds the dictionary which stores the most common words up to 3000\")\n",
        "def clean_Dict(root_dir):\n",
        "\n",
        "  new_dict = []\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
        "  for mail in emails:\n",
        "    with open(mail) as m:\n",
        "      for line in m:\n",
        "        words = line.split()\n",
        "        new_dict += words #Here the words and symbols are being added up\n",
        "\n",
        "  old_dict= Counter(new_dict)\n",
        "  list_to_remove = list(new_dict)\n",
        "\n",
        "  for item in list_to_remove:\n",
        "    if item.isalpha() == False:\n",
        "      del old_dict[item]\n",
        "    elif len(item) == 1:\n",
        "      del old_dict[item] #Here the non alpha numeric characters and single characters are being deleted\n",
        "  old_dict = old_dict.most_common(3000) #Storing the most common 3000 words\n",
        "  return old_dict"
      ],
      "metadata": {
        "id": "MbD9wieyVxjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a9b762-e37b-41a9-b0dc-a61aca8fa394"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here we are creating a function that builds the dictionary which stores the most common words up to 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Here we are creating a function that does the following: extracts feature columns and populates their column as well as analyze file names and determines whether they are spam.\")\n",
        "def extraction(mail_dir):\n",
        "\n",
        "  email = [os.path.join(mail_dir,file) for file in os.listdir(mail_dir)] #assigning the path of the mail files to the email variable\n",
        "  features_matrix = np.zeros((len(email),3000)) #features are being extracted from the email text length = 3000\n",
        "  train_labels = np.zeros(len(email)) \n",
        "  count = 1;\n",
        "  docID = 0;\n",
        "\n",
        "  for fil in email: #For loop is conditioning the function on what do to do with the columns\n",
        "    with open(fil) as file:\n",
        "      for i, line in enumerate(file):\n",
        "        if i ==2:\n",
        "          words = line.split()\n",
        "          for word in words:\n",
        "            wordID = 0\n",
        "            for i, d in enumerate(old_dict):\n",
        "              if d[0] == word:\n",
        "                wordID = i\n",
        "                features_matrix[docID,wordID] = words.count(word) \n",
        "\n",
        "      train_labels[docID] = 0;\n",
        "      filepathTokens = fil.split('/')\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
        "      \n",
        "      if lastToken.startswith(\"spmsg\"): #If condintional is being utilized to decipher whether an email is spam or not\n",
        "        train_labels[docID] = 1;\n",
        "        count = count + 1\n",
        "      docID = docID + 1\n",
        "\n",
        "  return features_matrix, train_labels"
      ],
      "metadata": {
        "id": "fTL2b6t7WKfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcdd68e9-92d4-4d42-bb86-6256e33660d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here we are creating a function that does the following: extracts feature columns and populates their column as well as analyze file names and determines whether they are spam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive') #Mounting drive so that we can access the email files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86746d0-4f4a-4bc2-fd07-3f840aa06e54",
        "id": "w-BCkcaBKPpm"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score.\"\"\"\n",
        "TRAIN_DIR = '/content/drive/My Drive/Colab Notebooks/Data/train-mails'\n",
        "TEST_DIR = '/content/drive/My Drive/Colab Notebooks/Data/test-mails'"
      ],
      "metadata": {
        "id": "KBHJnn-tKoPC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_dict = clean_Dict(TRAIN_DIR)#Passing TRAIN_DIR through the clean_Dict function \n",
        "\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extraction(TRAIN_DIR)#Passing TRAIN_DIR through extraction function\n",
        "test_features_matrix, test_labels = extraction(TEST_DIR)#Passing TEST_DIR through extraction function\n",
        "\n",
        "model = GaussianNB()#We are utilizing the Gaussian model for Naivee Bayes as we are performing a classification method to the email text\n",
        "\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels)\n",
        "print (\"Training completed\")\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(test_labels, predicted_labels))\n",
        "\n",
        "\"\"\"Completed scoring of model\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "iSYsQ_47WVpT",
        "outputId": "ed83a5fb-b441-4beb-c22d-81c0af01897e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Completed scoring of model'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}